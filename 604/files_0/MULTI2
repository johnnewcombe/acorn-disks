|\ multi2|L8|a24TYPEtask_ptr=^tcb;tcb=RECORD  { 174 bytes total }  rx_seq:Byte;  user_info:user_ptr;  {These 3 items copied into globals}  user_station:Word;  err:Byte;  SP_val:Word;       {Stack pointer. PC and IX register saved on stack}  main_rx:Std_rx;    {This holds the request received from the client}  aux_error:min_stringEND;state_table=ARRAY[1..nooftasks] OF RECORD  waiting_for:Byte;   {Number of blocks due from peripheral}  release_at:Byte     {Maximum number allowed to be outstanding                       if the task is to be restarted}END;COMMONuser:user_ptr; {These 3 globals are copies of task control block fields}usr_stn:Word;error:Byte;task:task_ptr; {these two are directly related}task_no:Byte;status:state_table;no_swap:Boolean; {Task must not swap if True}{There are further COMMONs, not connected with task control}|L12|a4|u2.4 Task controlThe system has now been described for accessing the separate and shared variables of the tasks; mechanisms are now required to control swapping between tasks, and to initialise the various pointers.As has already been described, no task swaps only occur when requested by the currently active task.  A task will suspend itself only when it has no more work to do immediately, usually because it is waiting for a disc or network transfer.  When a task suspends itself, the environment must be preserved until the task is re-started, and ideally the task should not be re-started until there is some work for it to do.  In addition, it should be arranged that no task has priority over any other.The task swap operation is initiated by a procedure call to a scheduling routine called Swap, which takes no parameters.  The compiler implements this as a simple subroutine jump; the IX register contains the address of the current stack frame and must be preserved, but any important values in the other registers will already have been preserved.  The scheduler therefore pushes the IX register onto the stack, and copies the three task-dependent global variables and the stack pointer into their respective task control block positions; this has stored the entire environment for that task.The scheduler now has to activate another task.  It considers the tasks in numerical order, starting with that numbered one greater than the current task, and the first one that is marked 'ready' will be re-started.  This means that the task just suspended will not be re-started until all the other tasks have had the opportunity to run, thus satisfying the requirement for no priority so far as the main processor is concerned.  It also implies a condition on the program for the I/O processor that it must not process the disc requests in such an order that any task can be locked out by requests from others.The operation of re-starting a task is straightforward : the task control block pointer and task number globals must be set up, the three globals and the stack pointer reloaded from the task control block.  The environment is now restored, and the scheduler exits by popping the IX register from the stack and executing a return instruction.The mechanism for indicating which tasks are ready requires consideration of the protocols used on the the I/O processor control channel.  There are two types of message sent to the I/O processor : those expecting instant response (such as a status enquiry) and requests for a disc transfer (which may be for a number of blocks).  The responses to a disc transfer request may come at any time in the future, and consist of a stream of numbers indicating the number of blocks which have become available since the last request.  In order to identify the mixture of replies coming back from the I/O processor, each byte is accompanied by the task number with which it is associated.  Replies to 'instant' requests are given task numbers which indicate the type of request, outside the range of normal task numbers.When a task calls the disc read/write routine, three things happen : the details of the request are sent to the I/O processor, the state table is updated, and Swap is called.  The state table records, for each task, the total number of blocks which are expected from the disc, and also the maximum number that can still be outstanding if the task is to be considered ready.  There is one polling routine to extract all replies from the communication latches.  If a reply has a task number outside the normal range, it is passed back to the caller, but disc transfer replies are processed directly : the number returned (which is the number of blocks that have become available) is subtracted from the state table entry for that task.The scheduler calls this polling routine frequently, to ensure that the state table is updated.  There should be no 'instant' requests pending when the scheduler is called : if the polling routine actually returns a result, an error has occured.  Tasks are 'ready' when the number of blocks outstanding is less than or equal to the maximum specified.  The mechanism to re-start tasks before the whole transfer is complete is provided for tasks which are returning many blocks of data to a client : the data has to be sent across the network in packets of limited size, so useful work can probably be done as soon as the first two blocks are available.It is intended that the same mechanism be used to hold up tasks waiting for reception from the network : the reception of a packet into the appropriately numbered receive area would cause the 'outstanding' count for that task (in the state table) to be decremented.  Unfortunately, this requires a modification to the network primitives, and it was not possible to arrange this within the project timescale.  The standard primitives require each task to poll for reception of a packet by calling a function which returns True if a packet has arrived (although the actual reception occurs under interrupt control, so the rate of polling is not important).  The main client response loop therefore looks like :|L8|a18|i5REPEAT {Main loop for FS tasks}  Rx_set(cmd_port,0,Addr(task^.mainRX),Sizeof(std_Rx),task_no);  REPEAT    Swap  UNTIL Rxread(task^.rx_seq,dummy_byte,usr_stn,dummy_word,task_no);{ Drops through here when a packet arrives on the FS command port }  CASE task^.mainRX.fn OF    0:Starcmd;    1:Save;    2:Load; {   other functions here.... }   ENDUNTIL False;END.|L12|iOther areas where a packet is expected from the client have a similar 'REPEAT Swap UNTIL Rx_read' loop.  This means that each task which is ready to receive a packet from the network incurs the overhead of being re-started, a function call with several parameters, and suspending again, for each cycle of task numbers by the scheduler.  As a result, it is not worthwhile to run with large numbers of tasks : with four tasks executing this loop, the response to single client requests was measureably slower than the single task version of the fileserver.  The intended solution should remove this overhead altogether, as the task selection loop of the scheduler is very tight.|gmulti3