|\ cache|u5. Directory cacheWith the overall improvement in performance provided by the new system, the inefficiencies in the system for accessing directory entries became more prominent than ever.  Many operations reference or update several directory entries at once, but for historic reasons (primarily lack of memory on earlier hosts) the fileserver had a very crude means of manipulating directories.  There was a single buffer capable of holding one block (there are 16 directory entries per block); to access any directory entry the whole of the enclosing block was read into memory, and if any update was made, the whole block was written out again.  This worked adequately well if the directories had few entries, but as soon as a directory exceeded 15 entries the efficiency became very poor.The sequence for looking up a name in a directory is : obtain the directory entry describing the directory to be searched, yielding the list of blocks containing the directory.  Obtain entry 0, and extract the entry number of the first entry in the chain for names beginning with the appropriate letter.  Obtain each entry in the chain, testing for the given name.  If a create operation had been specified, there will now be a need to read entry 0 to find a free entry, and then to update entry 0, the previous entry in the chain, the entry describing the directory and the account balance in block 0.Although this process would only require access to, typically, 8 entries, several are referenced more than once, and the split between blocks could easily be such that each reference required a different block to be brought into the buffer.  A similar problem concerns access to files which are longer than 16K, where the block list is stored separately from the file's directory entry : if a client makes a series of requests for small portions of the file, the fileserver has to read the directory entry to find the block list, and then has to read the block list itself, before responding to each request.  In both these cases, the ability to store information from more than one block in main memory would dramatically improve the performance.|a4|u5.1 Design considerationsThe solution is to keep a cache of directory information in main memory.  This could have been arranged as a cache of whole disc blocks, which would have involved minimal effort.  Unfortunately, there are only about 16K bytes of memory available (room for 16 blocks), and as there would be several tasks sharing the cache the chance of finding a desired block would be very low.The obvious alternative unit to store is a single directory entry.  This seems a satisfactory size, as more than 200 can be stored in the available space, and there is potential to recover some of the increased overheads by a suitable representation, since the indexing operation to find the n th entry in a block is only done once.  The penalty of a small cache unit, however, is that the time taken to search the cache could easily be greater than that taken to read in a whole block under the old system : it is worth using some of the memory for hash tables etc. at the cost of reducing the cache size.To maximise the indirect benefits of the new cache system, two features are desirable in the representation : a cache entry should be described by a pointer, rather than an array index or offset, and it should be possible to reference more than one entry at once.  The use of a pointer obviously saves index calculations, and the ability to hold more than one reference saves the cost of the cache search when returning to a recently used entry.There is a further question for overall design : the handling of modified entries.  The choice is between a system which marks cache entries as 'dirty', and leaves the write-back operation for later, or a policy that all cache entries are disposable, requiring the routine that makes an update to write the entry back to disc before releasing it.  The latter alternative was chosen here, for several reasons.  The cost of writing out the entry at the time that it is modified is fairly low, as it is not written directly to disc but into the main cache, under the management of the I/O processor.  In general, repeated updates to a single entry are rare, so little is gained by delaying work which has to be done eventually.As both the cache and the user table are stored on the Pascal heap, it would be posible to arrange for the trade-off between cache size and number of logged on users to be dynamic; this was not attempted here as the complexity of store management which would result was not considered worthwhile.  There is no reason why it could not be introduced at a later date.|a4|u5.2 Cache internalsThe primary requirement of the cache design is speed.  This means that two operations must be fast : searching the cache to find an existing entry, and selecting a spare entry to re-use if the desired entry is not found.Fast lookup is achieved by using a large hash table : a hash function of the requested disc address is computed, and indirection through the hash table yields a pointer to a chain of entries having this hash value.  The lookup routine will scan down that chain, comparing the full entry numbers of each entry; if the next entry pointer is nil, the end of the chain has been reached and the desired entry is not in the cache.  The chains are kept short by having a large hash table (the present implementation has rather more hash table slots than the cache holds entries), and the hash function is chosen to give different values for entries which are likely to be in the cache together : each entry of a directory will usually have a distinct hash value from all others in that directory.There is no reason to prefer to keep any particular entries in preference to others; it might be thought that the root directory or the library directory should be retained in preference to others, but users working entirely on their own files will need neither of these.  The simple policy of re-using the least recently referenced entry was therefore chosen.  This does not entirely solve the problem, however, as some task may still be holding a reference to the oldest entry : there is a need to maintain a reference count, and to re-use the oldest entry that has a zero reference count.  The case where all entries have non-zero reference counts is regarded as a fatal error : tasks should only hold references for a short time, and the maximum number which any task ever needs to hold at once is limited to four.  The minimum cache size is therefore four times the number of tasks.The age of entries is recorded by keeping them in a linked list; each time that an entry is requested, it is moved to the head of the list.  The length of the list and the need to remove entries from arbitrary positions, mean that a double-linked list with head and tail pointers is required : searching the whole length of the list each time would be unacceptably slow.|L8|a22The relevant declarations are :|i5TYPECache_ptr=^Cache_descrip;Cache_descrip=RECORD  which_ent:disc_address;  older,newer:cache_ptr;  {Next/previous entries in age chain}  next_ent:cache_ptr;     {next entry in hash chain}  ref_count:Byte;  {Count of tasks using the entry. Set to &80+n if in transit}  CASE (1..3) OF    1:(bytes:ARRAY [0..63] OF Byte);    2:(words:ARRAY [0..31] OF Word);    3:(dir_ent:Ent_type)END;VARcache_hash:ARRAY [byte] OF Cache_ptr;oldest:Cache_ptr;COMMONc_ent:Cache_ptr;  {This is the head of the age chain}|i|L12|gcache2